import vertexai
from google.auth import default
from vertexai.preview.generative_models import GenerativeModel
from google.api_core.exceptions import ResourceExhausted
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# Initialize Vertex AI
PROJECT_ID = "mlds-cap-2024-lexlead-advisor"  # Replace with your project ID
REGION = "us-central1"
vertexai.init(project=PROJECT_ID, location=REGION)

# Authentication
credentials, project = default()

# Load the Gemini model
generative_model_gemini_15_pro = GenerativeModel("gemini-1.5-pro-002")

@retry(
    stop=stop_after_attempt(7),  # Retry up to 7 attempts
    wait=wait_exponential(multiplier=1, min=5, max=120),  # Min wait 5s, exponential up to 120s
    retry=retry_if_exception_type(ResourceExhausted)
)
def generate_response(model, prompt):
    """
    Generates a response using the specified model with retry logic.

    Args:
        model (GenerativeModel): The generative AI model to use.
        prompt (str): The input prompt for generating a response.

    Returns:
        str: The text response generated by the model or an error message if no valid response is found.
    """
    try:
        response = model.generate_content([prompt])
        response_dict = response.to_dict()

        if 'candidates' in response_dict and response_dict['candidates']:
            candidate = response_dict['candidates'][0]
            if 'content' in candidate and 'parts' in candidate['content'] and candidate['content']['parts']:
                text_response = candidate['content']['parts'][0].get('text', '').strip()
                return text_response if text_response else "Error: No text found in response."
            else:
                return "Error: No valid content or parts found in candidate."
        else:
            return "Error: No valid candidates found in response."
    except Exception as e:
        raise

def should_use_rag_with_gemini(question_or_category):
    """
    Determines if RAG (Retrieval-Augmented Generation) is useful based on the input question or category.

    Args:
        question_or_category (str): The input question or category to evaluate.

    Returns:
        bool: True if RAG is useful, False otherwise.
    """
    prompt = f"""
    Based on the following question or category, determine if RAG (Retrieval-Augmented Generation) is useful. 
    If the question requires recalling external legal information or citation, RAG is useful.
    If the question is focused on interpretation or understanding without external references, RAG is not useful.
    
    Question/Category: {question_or_category}
    
    Answer with 'Yes' or 'No' based on whether RAG is useful.
    """
    response = generate_response(generative_model_gemini_15_pro, prompt)
    print(f"Reason-2: Is RAG useful for this task?\nAnswer-2: {'Yes' if 'yes' in response.lower() else 'No'}")
    return "yes" in response.lower()

def should_use_raptor(question):
    """
    Determines if the question requires high-level information best provided by Raptor.

    Args:
        question (str): The input question to evaluate.

    Returns:
        str: 'Raptor' if Raptor is needed, otherwise 'RAG'.
    """
    prompt = f"""
    Determine if the following question requires high-level information that is best provided by Raptor, or if regular RAG is sufficient.
    
    Examples:
    1. Question: "Provide a summary of the legal landscape for environmental regulations in Illinois."
       Answer: "Raptor"
    2. Question: "What are the key requirements of the Illinois Clean Air Act?"
       Answer: "RAG"
    
    Question: {question}
    
    Answer with 'Raptor' or 'RAG' based on the need for high-level information.
    """
    response = generate_response(generative_model_gemini_15_pro, prompt)
    print(f"Reason-5: Does the question need high-level information?\nAnswer-5: {response.strip()}")
    return response.strip()

def classify_question_category(question):
    """
    Classifies the question into a predefined legal category.

    Args:
        question (str): The input question to classify.

    Returns:
        str: The determined category of the question.
    """
    prompt = f"""
    Classify the following question into one of the categories: issue-spotting, rule-recall, rule-application, 
    rule-conclusion, interpretation, rhetorical-understanding, out-of-scope.
    
    Question: {question}
    
    Provide the most appropriate category for the question.
    """
    response = generate_response(generative_model_gemini_15_pro, prompt)
    print(f"Reason-1: What is the category of the question?\nAnswer-1: {response.strip()}")
    return response.strip() if "Error" not in response else response

def classify_difficulty(question, category):
    """
    Classifies the difficulty of the question based on its category.

    Args:
        question (str): The input question to evaluate.
        category (str): The category of the question.

    Returns:
        str: The difficulty level ('Easy', 'Medium', 'Hard') or an error message.
    """
    category_performance = {
        "rule-recall": 30,
        "rule-application": 50,
        "rule-conclusion": 40,
        "interpretation": 80,
        "rhetorical-understanding": 70,
        "issue-spotting": 60,
    }

    category_difficulty = "Hard"
    if category in category_performance:
        performance = category_performance[category]
        if performance >= 70:
            category_difficulty = "Easy"
        elif performance >= 50:
            category_difficulty = "Medium"

    prompt = f"""
    Classify the difficulty of the question based on its category:
    Question: "{question}"
    Category: {category}

    Based on the question and category, classify the difficulty as **Easy**, **Medium**, or **Hard**.
    """
    response = generate_response(generative_model_gemini_15_pro, prompt)
    print(f"Reason-3: What is the difficulty level of the question?\nAnswer-3: {response.strip()}")
    return response.strip() if "Error" not in response else "Error: Unable to classify difficulty."

def evaluate_question(question):
    """
    Evaluates a question by classifying its category, determining if RAG is useful, and suggesting a strategy.

    Args:
        question (str): The input question to evaluate.

    Returns:
        tuple: A tuple containing the strategy, category, RAG usefulness, and difficulty classification.
    """
    category = classify_question_category(question)
    is_rag_useful = should_use_rag_with_gemini(category)
    difficulty_response = classify_difficulty(question, category)
    is_illinois_law = "Illinois" in question

    print(f"Reason-4: Is the question related to Illinois law?\nAnswer-4: {'Yes' if is_illinois_law else 'No'}")

    if is_rag_useful:
        strategy = "LLM + Raptor" if is_illinois_law else "LLM + Google Search"
    else:
        strategy = "LLM" if "Easy" in difficulty_response else "LLM + Google Search"

    print(f"Final Strategy: {strategy}")
    return strategy, category, is_rag_useful, difficulty_response
